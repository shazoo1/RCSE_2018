{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model reliability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The basics\n",
    "\n",
    "Suppose we have fit a model to a set of data. In doing so, we have determined values for the free parameters of the model. It is useful to know how certain we can be with respect to the specific parameter values that we have obtained.\n",
    "\n",
    "The reason that there is uncertainty at all has to do with the distinction between the sample and the population. We are not intrinsically interested in the model parameters that best describe a given sample of data, but rather we are interested in the model parameters that best describe the population from which the sample is drawn.\n",
    "\n",
    "Due to sampling error, we can expect some variability in the mean of sample drawn from a distribution and we therefore put error bars on the mean of a given sample of data. Likewise, dut to sampling error, we can expect some variability in the model parameters that describe a sample of data. We therefore put error bars on model parameters estimated from a given sample of data.\n",
    "\n",
    "\n",
    "We define **model reliability** as the stability, with respect to sampling error, of the parameters of a fitted model. The more data we collect, the more reliable the model parameters will be.\n",
    "\n",
    "![%D0%B8%D0%B7%D0%BE%D0%B1%D1%80%D0%B0%D0%B6%D0%B5%D0%BD%D0%B8%D0%B5.png](attachment:%D0%B8%D0%B7%D0%BE%D0%B1%D1%80%D0%B0%D0%B6%D0%B5%D0%BD%D0%B8%D0%B5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Error bars on parameter estimates\n",
    "\n",
    "\n",
    "To calculate error bars on parameter estimates, we once again turn to the nonparametric technique of **bootstraping**. The idea is simple: draw bootstrap samples from the observed data, fit the model of interest to each bootstrap sample, and examine the distribution of parameter estimates across the various bootstrap iterations.\n",
    "\n",
    "Note that dependng on our goals, we might be interested in something other than the individual model parameters. For example, we might be interested in some metric computed from several parameters or we might be interested in the overall function described by a given model.  Bootstrapping handles these scenarion nicely: compute the quantity of interest at each bootstrap iteration, and then look at the discribution of this quantity across the various iterations.\n",
    "\n",
    "![%D0%B8%D0%B7%D0%BE%D0%B1%D1%80%D0%B0%D0%B6%D0%B5%D0%BD%D0%B8%D0%B5.png](attachment:%D0%B8%D0%B7%D0%BE%D0%B1%D1%80%D0%B0%D0%B6%D0%B5%D0%BD%D0%B8%D0%B5.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Parameter estimates may have correlated errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " For models with more than one parameter, the sampling error is a multidimentional discribution. Thus, putting an error bar on individual parameters may not provide a complete characterization of the sampling error (since there may be interactions between parameters).\n",
    "\n",
    "![%D0%B8%D0%B7%D0%BE%D0%B1%D1%80%D0%B0%D0%B6%D0%B5%D0%BD%D0%B8%D0%B5.png](attachment:%D0%B8%D0%B7%D0%BE%D0%B1%D1%80%D0%B0%D0%B6%D0%B5%D0%BD%D0%B8%D0%B5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le'ts take a simple example: Suppose we have a linear regression model with two regressors that are highly correlated. In this case, an increase in the weight associated with on regressor can be counteracted by a degrease in the weight associated with the other regressor. So, in a sense, it is easy for the weights to vary in an anti-correlated fashion. \n",
    "\n",
    "Examining variability in weights estimated from different samples, we will find that the error on oen weight negatively correlates with the error on the other weight."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A useful way to think about correlated errors is in terms of the error surface. If the error surface is shallow along certain directions in the parameter space, then errors in the parameter estimates will tend to be correlated along these same directions.\n",
    "\n",
    "The weights estimated for correlated regressors will tend to have low reliability. However, even though the individual weights are unreliable, the overall model can still be reliable. That is, given estimates of the individual weights, we can construct the model fit as a weighted sum of the regressors (using the estimated weights), and this model fit can still have high stability (across samples from the population)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. The distinction between accuracy and reliability\n",
    "\n",
    "Accuracy and reliability are related but are not the same. Accuracy corresponds to how far a fitted model is away from the true model, whereas reliability corresponds to how stable the fitted model is.\n",
    "\n",
    "There are four possible scenarious:\n",
    "![%D0%B8%D0%B7%D0%BE%D0%B1%D1%80%D0%B0%D0%B6%D0%B5%D0%BD%D0%B8%D0%B5.png](attachment:%D0%B8%D0%B7%D0%BE%D0%B1%D1%80%D0%B0%D0%B6%D0%B5%D0%BD%D0%B8%D0%B5.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
